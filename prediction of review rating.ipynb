{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn, pandas\n",
    "import library\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA, TruncatedSVD, SparsePCA\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import average_precision_score, recall_score, f1_score, precision_score, classification_report\n",
    "from sklearn.random_projection import sparse_random_matrix\n",
    "\n",
    "df_train = pandas.read_csv('training.txt', delimiter = \"\\t\", names = [\"Review\", \"Rating\"])\n",
    "df_test = pandas.read_csv('test.txt', delimiter = \"\\t\", names = [\"Review\", \"Rating\"])\n",
    "\n",
    "text = df_train.Review\n",
    "rating = df_train.Rating\n",
    "\n",
    "text_test = df_test.Review\n",
    "rating_test = df_test.Rating\n",
    "\n",
    "X_train, y_train = library.balance_classes(text, rating)\n",
    "X_test, y_test = library.balance_classes(text_test, rating_test)\n",
    "\n",
    "#Learn vocabulary and idf from training set\n",
    "vector = TfidfVectorizer().fit(X_train)\n",
    "# transform the documents in the training data to a document-term matrix\n",
    "X_train_vectorized = vector.transform(X_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: \n",
    "### Run\t5-fold\tCross\tValidation\ton\tthe\ttraining.txt\tusing\t5 learning\talgorithms:  MLPClassifier(Neural network), MultinomialNB, LogisticRegression, AdaBoostClassifier, and SVC .  Report\tthe\taverage-precision,\taverage-recall\tand\taverage-F1-scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Classifier, the optimal setting: hidden size (70, 50, 100), accuracy is 0.54"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hiden size:  (100, 100)  average cv score: 0.49569963290009866\n",
      "hiden size:  (100, 100)  average precision score: 0.5188773805546275\n",
      "hiden size:  (100, 100)  average recall score: 0.5188773805546275\n",
      "hiden size:  (100, 100)  average f1 score: 0.5188773805546275\n",
      "hiden size:  (70, 50, 100)  average cv score: 0.5269095480941693\n",
      "hiden size:  (70, 50, 100)  average precision score: 0.5399264951553625\n",
      "hiden size:  (70, 50, 100)  average recall score: 0.5399264951553625\n",
      "hiden size:  (70, 50, 100)  average f1 score: 0.5399264951553625\n",
      "hiden size:  (5, 2)  average cv score: 0.4717906923660453\n",
      "hiden size:  (5, 2)  average precision score: 0.49381891079184764\n",
      "hiden size:  (5, 2)  average recall score: 0.49381891079184764\n",
      "hiden size:  (5, 2)  average f1 score: 0.49381891079184764\n",
      "hiden size:  (5,)  average cv score: 0.4872504868403995\n",
      "hiden size:  (5,)  average precision score: 0.4827931840962245\n",
      "hiden size:  (5,)  average recall score: 0.4827931840962245\n",
      "hiden size:  (5,)  average f1 score: 0.48279318409622446\n"
     ]
    }
   ],
   "source": [
    "#Neural Network model\n",
    "hsize = [(100,100), (70,50,100),(5,2),(5,)]\n",
    "\n",
    "model_NN = []\n",
    "for h in hsize:\n",
    "    classifier_NN = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes= h, random_state=1)\n",
    "    model_NN.append(classifier_NN)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_vectorized,y_train,test_size = 0.3)\n",
    "\n",
    "count = 0\n",
    "for model in model_NN:\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    pred_NN = model.predict(X_val)\n",
    "    scores = cross_val_score(model, X_train, y_train, cv = 5)\n",
    "    prescore = precision_score(pred_NN, y_val, average='micro')\n",
    "    rescore = recall_score(pred_NN, y_val, average='micro')\n",
    "    f1score = f1_score(pred_NN, y_val, average='micro')\n",
    "    \n",
    "    print(\"hiden size: \", hsize[count], ' average cv score:', sum(scores)/len(scores))\n",
    "    print(\"hiden size: \", hsize[count], ' average precision score:', prescore)\n",
    "    print(\"hiden size: \", hsize[count], ' average recall score:', rescore)\n",
    "    print(\"hiden size: \", hsize[count], ' average f1 score:', f1score)\n",
    "    count +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier, average cv score: 0.43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Classifier: \n",
      "average cv score: 0.42736842105263156\n",
      "average precision score: 0.506\n",
      "average recall score: 0.506\n",
      "average f1 score: 0.506\n"
     ]
    }
   ],
   "source": [
    "#naieve Base model\n",
    "classifier_NB = MultinomialNB()\n",
    "\n",
    "classifier_NB.fit(X_train_vectorized, y_train)\n",
    "    \n",
    "pred_NB = classifier_NB.predict(vector.transform(X_test))\n",
    "scores = cross_val_score(model, X_train_vectorized, y_train, cv = 5)\n",
    "prescore = precision_score(pred_NB, y_test, average='micro')\n",
    "rescore = recall_score(pred_NB, y_test, average='micro')\n",
    "f1score = f1_score(pred_NB, y_test, average='micro')\n",
    "print('Naive Bayes Classifier: ')\n",
    "print('average cv score:', sum(scores)/len(scores))\n",
    "print('average precision score:', prescore)\n",
    "print('average recall score:', rescore)\n",
    "print('average f1 score:', f1score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression, the optimal setting: ('l2', 1.0), average score is  0.535"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('l1', 1.0)  average cv score: 0.4669674185463659\n",
      "('l1', 1.0)  average precision score: 0.517\n",
      "('l1', 1.0)  average recall score: 0.517\n",
      "('l1', 1.0)  average f1 score: 0.517\n",
      "('l1', 2.0)  average cv score: 0.4606516290726817\n",
      "('l1', 2.0)  average precision score: 0.509\n",
      "('l1', 2.0)  average recall score: 0.509\n",
      "('l1', 2.0)  average f1 score: 0.509\n",
      "('l1', 3.0)  average cv score: 0.4542355889724311\n",
      "('l1', 3.0)  average precision score: 0.49\n",
      "('l1', 3.0)  average recall score: 0.49\n",
      "('l1', 3.0)  average f1 score: 0.49\n",
      "('l1', 4.0)  average cv score: 0.44551378446115286\n",
      "('l1', 4.0)  average precision score: 0.498\n",
      "('l1', 4.0)  average recall score: 0.498\n",
      "('l1', 4.0)  average f1 score: 0.498\n",
      "('l2', 1.0)  average cv score: 0.4670676691729323\n",
      "('l2', 1.0)  average precision score: 0.535\n",
      "('l2', 1.0)  average recall score: 0.535\n",
      "('l2', 1.0)  average f1 score: 0.535\n",
      "('l2', 2.0)  average cv score: 0.46335839598997497\n",
      "('l2', 2.0)  average precision score: 0.527\n",
      "('l2', 2.0)  average recall score: 0.527\n",
      "('l2', 2.0)  average f1 score: 0.527\n",
      "('l2', 3.0)  average cv score: 0.4608521303258145\n",
      "('l2', 3.0)  average precision score: 0.521\n",
      "('l2', 3.0)  average recall score: 0.521\n",
      "('l2', 3.0)  average f1 score: 0.521\n",
      "('l2', 4.0)  average cv score: 0.4591478696741854\n",
      "('l2', 4.0)  average precision score: 0.514\n",
      "('l2', 4.0)  average recall score: 0.514\n",
      "('l2', 4.0)  average f1 score: 0.514\n"
     ]
    }
   ],
   "source": [
    "#Logistic regression model\n",
    "penaty = ['l1', 'l2']\n",
    "C = [1.0, 2.0, 3.0, 4.0]\n",
    "\n",
    "model_LR = []\n",
    "pc = []\n",
    "for p in penaty:\n",
    "    for c in C:\n",
    "        classifier_LR = LogisticRegression(penalty = p, C = c)\n",
    "        model_LR.append(classifier_LR)\n",
    "        pc.append((p,c))\n",
    "\n",
    "count = 0\n",
    "for model in model_LR:\n",
    "    model.fit(X_train_vectorized, y_train)\n",
    "    \n",
    "    pred_NN = model.predict(vector.transform(X_test))\n",
    "    scores = cross_val_score(model, X_train_vectorized, y_train, cv = 5)\n",
    "    prescore = precision_score(pred_NN, y_test, average='micro')\n",
    "    rescore = recall_score(pred_NN, y_test, average='micro')\n",
    "    f1score = f1_score(pred_NN, y_test, average='micro')\n",
    "    \n",
    "    print(pc[count], ' average cv score:', sum(scores)/len(scores))\n",
    "    print(pc[count], ' average precision score:', prescore)\n",
    "    print(pc[count], ' average recall score:', rescore)\n",
    "    print(pc[count], ' average f1 score:', f1score)\n",
    "    count +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboosting, the optimal setting: n_estimators = 50, average score is 0.47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators:  10  average cv score: 0.39458646616541354\n",
      "n_estimators:  10  average precision score: 0.41\n",
      "n_estimators:  10  average recall score: 0.41\n",
      "n_estimators:  10  average f1 score: 0.41\n",
      "n_estimators:  20  average cv score: 0.41563909774436086\n",
      "n_estimators:  20  average precision score: 0.462\n",
      "n_estimators:  20  average recall score: 0.462\n",
      "n_estimators:  20  average f1 score: 0.462\n",
      "n_estimators:  50  average cv score: 0.4140350877192982\n",
      "n_estimators:  50  average precision score: 0.47\n",
      "n_estimators:  50  average recall score: 0.47\n",
      "n_estimators:  50  average f1 score: 0.47\n",
      "n_estimators:  100  average cv score: 0.4109273182957393\n",
      "n_estimators:  100  average precision score: 0.443\n",
      "n_estimators:  100  average recall score: 0.443\n",
      "n_estimators:  100  average f1 score: 0.443\n"
     ]
    }
   ],
   "source": [
    "#adaboosting model\n",
    "n_estimators = [10, 20, 50, 100]\n",
    "\n",
    "model_AB = []\n",
    "for n in n_estimators:\n",
    "    classifier_AB = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2), n_estimators=n)\n",
    "    model_AB.append(classifier_AB)\n",
    "\n",
    "count = 0\n",
    "for model in model_AB:\n",
    "    model.fit(X_train_vectorized, y_train)\n",
    "    \n",
    "    pred = model.predict(vector.transform(X_test))\n",
    "    scores = cross_val_score(model, X_train_vectorized, y_train, cv = 5)\n",
    "    prescore = precision_score(pred, y_test, average='micro')\n",
    "    rescore = recall_score(pred, y_test, average='micro')\n",
    "    f1score = f1_score(pred, y_test, average='micro')\n",
    "    \n",
    "    print(\"n_estimators: \", n_estimators[count], ' average cv score:', sum(scores)/len(scores))\n",
    "    print(\"n_estimators: \", n_estimators[count], ' average precision score:', prescore)\n",
    "    print(\"n_estimators: \", n_estimators[count], ' average recall score:', rescore)\n",
    "    print(\"n_estimators: \", n_estimators[count], ' average f1 score:', f1score)\n",
    "    count +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM, the opotimal setting: (1.0, 'linear') , average score is 0.528"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.0, 'rbf')  average cv score: 0.3625062656641604\n",
      "(1.0, 'rbf')  average precision score: 0.396\n",
      "(1.0, 'rbf')  average recall score: 0.396\n",
      "(1.0, 'rbf')  average f1 score: 0.396\n",
      "(1.0, 'linear')  average cv score: 0.4719799498746868\n",
      "(1.0, 'linear')  average precision score: 0.528\n",
      "(1.0, 'linear')  average recall score: 0.528\n",
      "(1.0, 'linear')  average f1 score: 0.528\n",
      "(1.0, 'poly')  average cv score: 0.2693734335839599\n",
      "(1.0, 'poly')  average precision score: 0.301\n",
      "(1.0, 'poly')  average recall score: 0.301\n",
      "(1.0, 'poly')  average f1 score: 0.301\n",
      "(1.0, 'sigmoid')  average cv score: 0.36260651629072677\n",
      "(1.0, 'sigmoid')  average precision score: 0.396\n",
      "(1.0, 'sigmoid')  average recall score: 0.396\n",
      "(1.0, 'sigmoid')  average f1 score: 0.396\n",
      "(2.0, 'rbf')  average cv score: 0.3625062656641604\n",
      "(2.0, 'rbf')  average precision score: 0.396\n",
      "(2.0, 'rbf')  average recall score: 0.396\n",
      "(2.0, 'rbf')  average f1 score: 0.396\n",
      "(2.0, 'linear')  average cv score: 0.46466165413533833\n",
      "(2.0, 'linear')  average precision score: 0.515\n",
      "(2.0, 'linear')  average recall score: 0.515\n",
      "(2.0, 'linear')  average f1 score: 0.515\n",
      "(2.0, 'poly')  average cv score: 0.2687719298245614\n",
      "(2.0, 'poly')  average precision score: 0.306\n",
      "(2.0, 'poly')  average recall score: 0.306\n",
      "(2.0, 'poly')  average f1 score: 0.306\n",
      "(2.0, 'sigmoid')  average cv score: 0.36260651629072677\n",
      "(2.0, 'sigmoid')  average precision score: 0.396\n",
      "(2.0, 'sigmoid')  average recall score: 0.396\n",
      "(2.0, 'sigmoid')  average f1 score: 0.396\n",
      "(3.0, 'rbf')  average cv score: 0.3625062656641604\n",
      "(3.0, 'rbf')  average precision score: 0.396\n",
      "(3.0, 'rbf')  average recall score: 0.396\n",
      "(3.0, 'rbf')  average f1 score: 0.396\n",
      "(3.0, 'linear')  average cv score: 0.45513784461152884\n",
      "(3.0, 'linear')  average precision score: 0.517\n",
      "(3.0, 'linear')  average recall score: 0.517\n",
      "(3.0, 'linear')  average f1 score: 0.517\n",
      "(3.0, 'poly')  average cv score: 0.26897243107769425\n",
      "(3.0, 'poly')  average precision score: 0.304\n",
      "(3.0, 'poly')  average recall score: 0.304\n",
      "(3.0, 'poly')  average f1 score: 0.304\n",
      "(3.0, 'sigmoid')  average cv score: 0.36260651629072677\n",
      "(3.0, 'sigmoid')  average precision score: 0.396\n",
      "(3.0, 'sigmoid')  average recall score: 0.396\n",
      "(3.0, 'sigmoid')  average f1 score: 0.396\n",
      "(4.0, 'rbf')  average cv score: 0.3625062656641604\n",
      "(4.0, 'rbf')  average precision score: 0.396\n",
      "(4.0, 'rbf')  average recall score: 0.396\n",
      "(4.0, 'rbf')  average f1 score: 0.396\n",
      "(4.0, 'linear')  average cv score: 0.451829573934837\n",
      "(4.0, 'linear')  average precision score: 0.514\n",
      "(4.0, 'linear')  average recall score: 0.514\n",
      "(4.0, 'linear')  average f1 score: 0.514\n",
      "(4.0, 'poly')  average cv score: 0.26917293233082706\n",
      "(4.0, 'poly')  average precision score: 0.302\n",
      "(4.0, 'poly')  average recall score: 0.302\n",
      "(4.0, 'poly')  average f1 score: 0.302\n",
      "(4.0, 'sigmoid')  average cv score: 0.36260651629072677\n",
      "(4.0, 'sigmoid')  average precision score: 0.396\n",
      "(4.0, 'sigmoid')  average recall score: 0.396\n",
      "(4.0, 'sigmoid')  average f1 score: 0.396\n"
     ]
    }
   ],
   "source": [
    "#SVC model\n",
    "C1 = [1.0, 2.0, 3.0, 4.0]\n",
    "kernel = ['rbf', 'linear', 'poly', 'sigmoid']\n",
    "\n",
    "model_svm = []\n",
    "kc = []\n",
    "for c in C1:\n",
    "    for k in kernel:\n",
    "        classifier_SVC = SVC(C=c, kernel = k)\n",
    "        model_svm.append(classifier_SVC)\n",
    "        kc.append((c,k))\n",
    "\n",
    "count = 0\n",
    "for model in model_svm:\n",
    "    model.fit(X_train_vectorized, y_train)\n",
    "    \n",
    "    pred = model.predict(vector.transform(X_test))\n",
    "    scores = cross_val_score(model, X_train_vectorized, y_train, cv = 5)\n",
    "    prescore = precision_score(pred, y_test, average='micro')\n",
    "    rescore = recall_score(pred, y_test, average='micro')\n",
    "    f1score = f1_score(pred, y_test, average='micro')\n",
    "    \n",
    "    print(kc[count], ' average cv score:', sum(scores)/len(scores))\n",
    "    print(kc[count], ' average precision score:', prescore)\n",
    "    print(kc[count], ' average recall score:', rescore)\n",
    "    print(kc[count], ' average f1 score:', f1score)\n",
    "    count +=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: \n",
    "## 1. Perform feature selection using PCA and re-run the algorithms with their optimal settings\n",
    "## 2. But PCA does not support sparse input, therefore here I am using TruncatedSVD instead to reduce the dimentionality.\n",
    "## Result:\n",
    "### 1. n_components increases, the cross validation score is higher.\n",
    "### 2. In this example, n_component = 10, and model is neural network, the score is the highest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_component:  2  average cv score: 0.24010025062656642\n",
      "n_component:  2  average cv score: 0.2218546365914787\n",
      "n_component:  2  average cv score: 0.2336842105263158\n",
      "n_component:  2  average cv score: 0.2270676691729323\n",
      "n_component:  3  average cv score: 0.3237092731829574\n",
      "n_component:  3  average cv score: 0.2771929824561404\n",
      "n_component:  3  average cv score: 0.29834586466165414\n",
      "n_component:  3  average cv score: 0.2744862155388471\n",
      "n_component:  4  average cv score: 0.3651127819548872\n",
      "n_component:  4  average cv score: 0.3413533834586466\n",
      "n_component:  4  average cv score: 0.3335338345864662\n",
      "n_component:  4  average cv score: 0.34205513784461156\n",
      "n_component:  5  average cv score: 0.361203007518797\n",
      "n_component:  5  average cv score: 0.351077694235589\n",
      "n_component:  5  average cv score: 0.34135338345864663\n",
      "n_component:  5  average cv score: 0.35328320802005014\n",
      "n_component:  10  average cv score: 0.41423558897243107\n",
      "n_component:  10  average cv score: 0.3991979949874687\n",
      "n_component:  10  average cv score: 0.3771428571428571\n",
      "n_component:  10  average cv score: 0.4091228070175439\n",
      "n_component:  50  average cv score: 0.47017543859649125\n",
      "n_component:  50  average cv score: 0.4535338345864662\n",
      "n_component:  50  average cv score: 0.4107268170426065\n",
      "n_component:  50  average cv score: 0.4553383458646617\n",
      "n_component:  100  average cv score: 0.4867167919799499\n",
      "n_component:  100  average cv score: 0.45473684210526316\n",
      "n_component:  100  average cv score: 0.41142857142857137\n",
      "n_component:  100  average cv score: 0.4598496240601504\n"
     ]
    }
   ],
   "source": [
    "#classifier model with optimal settings\n",
    "classifier_NN = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes= (70, 50, 100), random_state=1)\n",
    "classifier_NB = MultinomialNB()\n",
    "classifier_LR = LogisticRegression(penalty = 'l2', C = 1.0)\n",
    "classifier_AB = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2), n_estimators=50)\n",
    "classifier_SVC = SVC(C=1.0, kernel = 'linear')\n",
    "\n",
    "models = [classifier_NN, classifier_LR,classifier_AB, classifier_SVC]\n",
    "n_component = [2, 3, 4, 5, 10, 50, 100]\n",
    "for n in n_component:\n",
    "    svd = TruncatedSVD(n_components=n)\n",
    "    svd.fit(X_train_vectorized)\n",
    "    X = svd.transform(X_train_vectorized)\n",
    "\n",
    "    for model in models:\n",
    "        model.fit(X, y_train)\n",
    "        scores = cross_val_score(model, X, y_train, cv = 5)\n",
    "        print(\"n_component: \", n,' average cv score:', sum(scores)/len(scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: \n",
    "## 1. Sentiment words processing\n",
    "## 2. Use sentiment dataset positive.txt and netgative.txt to filter review text\n",
    "## 3. \tEvaluate the algorithms on test.txt dataset\n",
    "\n",
    "# Result: the accuracy doesn't become better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Have', 'only', 'done', 'tapas', 'here.', '\\xa0', 'Must.', '\\xa0Get.', '\\xa0Ham', 'croquettes.', '\\xa0', 'Those', 'and', 'the', 'stuffed', 'plantains.', '\\xa0Lollipop', 'chicken', 'good,', 'ribs', 'good,', 'Cuban', 'sandwich', 'ace,', 'empanadas', 'solid.', '\\xa0Reasonably', 'priced,', 'service', 'attentive,', 'nice', 'walkway', 'to', 'the', 'restaurant.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laqin/Work/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Review  Rating\n",
      "0                                               nice     4.0\n",
      "1  amazing attractively generous happy impressive...     5.0\n",
      "2       attentive enough like nice ready loud scream     3.0\n",
      "3                                    good liked loud     4.0\n",
      "4                                     fun lead right     4.0\n"
     ]
    }
   ],
   "source": [
    "import sklearn, pandas\n",
    "import library\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "      \n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    " \n",
    "# load the document\n",
    "filename = 'positive-words.txt'\n",
    "text_p = load_doc(filename)\n",
    "filename = 'negative-words.txt'\n",
    "text_n = load_doc(filename)\n",
    "# merge two texts into one text\n",
    "text = text_p+text_n\n",
    "\n",
    "tokens = text.split()\n",
    "\n",
    "a = df_test[\"Review\"][0]\n",
    "y = a.split(\" \")\n",
    "print(y)\n",
    "\n",
    "l = []\n",
    "for i in range(len(df_test)):\n",
    "    a = df_test[\"Review\"][i]\n",
    "    for n in tokens:\n",
    "        if n in a.split(\" \"):\n",
    "            l.append(n)\n",
    "    df_test[\"Review\"][i] = \" \".join(l)\n",
    "    l = []\n",
    "\n",
    "print(df_test.head())\n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average cv score: 0.3390121411120486\n",
      "average precision score: 0.35\n",
      "average recall score: 0.35\n",
      "average f1 score: 0.35\n",
      "average cv score: 0.3972283190137029\n",
      "average precision score: 0.4066666666666667\n",
      "average recall score: 0.4066666666666667\n",
      "average f1 score: 0.4066666666666667\n",
      "average cv score: 0.408597390891563\n",
      "average precision score: 0.38333333333333336\n",
      "average recall score: 0.38333333333333336\n",
      "average f1 score: 0.38333333333333336\n",
      "average cv score: 0.30872142042354805\n",
      "average precision score: 0.2966666666666667\n",
      "average recall score: 0.2966666666666667\n",
      "average f1 score: 0.2966666666666667\n",
      "average cv score: 0.4142724404796561\n",
      "average precision score: 0.3566666666666667\n",
      "average recall score: 0.3566666666666667\n",
      "average f1 score: 0.35666666666666674\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "text_test_new = df_test.Review\n",
    "rating_test_new = df_test.Rating\n",
    "\n",
    "X_test_new, y_test_new = library.balance_classes(text_test_new, rating_test_new)\n",
    "#Learn vocabulary and idf from training set\n",
    "vector = TfidfVectorizer().fit(X_test_new)\n",
    "# transform the documents in the training data to a document-term matrix\n",
    "X_test_vectorized = vector.transform(X_test_new)\n",
    "\n",
    "#classifier model with optimal settings\n",
    "classifier_NN = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes= (70, 50, 100), random_state=1)\n",
    "classifier_NB = MultinomialNB()\n",
    "classifier_LR = LogisticRegression(penalty = 'l2', C = 1.0)\n",
    "classifier_AB = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2), n_estimators=50)\n",
    "classifier_SVC = SVC(C=1.0, kernel = 'linear')\n",
    "\n",
    "models = [classifier_NN, classifier_NB, classifier_LR,classifier_AB, classifier_SVC]\n",
    "\n",
    "X_train1, X_val1, y_train1, y_val1 = train_test_split(X_test_vectorized, y_test_new,test_size = 0.3)\n",
    "\n",
    "#Output the precision, recall and f1-score\n",
    "for model in models:\n",
    "    model.fit(X_train1, y_train1)  \n",
    "    pred = model.predict(X_val1)\n",
    "    scores = cross_val_score(model, X_train1, y_train1, cv = 5)\n",
    "    prescore = precision_score(pred, y_val1, average='micro')\n",
    "    rescore = recall_score(pred, y_val1, average='micro')\n",
    "    f1score = f1_score(pred, y_val1, average='micro')\n",
    "\n",
    "    print('average cv score:', sum(scores)/len(scores))\n",
    "    print('average precision score:', prescore)\n",
    "    print('average recall score:', rescore)\n",
    "    print('average f1 score:', f1score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4\n",
    "## 1. Perform evaluation on\tthe\ttest dataset using the optimal parameter settings that\t were obtained from the training set\n",
    "## 2. Report its precision, recall and f-scores\n",
    "## 3. For the best performing algorithm(Logistic Regression), compute precision and recall for every rating score separately\n",
    "## Which\ttypes\tof\treviews\twere\tthe\thardest\tto\tpredict?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average cv score: 0.4927638277906814\n",
      "average precision score: 0.4666666666666667\n",
      "average recall score: 0.4666666666666667\n",
      "average f1 score: 0.4666666666666667\n",
      "average cv score: 0.502825343795269\n",
      "average precision score: 0.5\n",
      "average recall score: 0.5\n",
      "average f1 score: 0.5\n",
      "average cv score: 0.502928277174745\n",
      "average precision score: 0.5133333333333333\n",
      "average recall score: 0.5133333333333333\n",
      "average f1 score: 0.5133333333333333\n",
      "average cv score: 0.3487336310415127\n",
      "average precision score: 0.37666666666666665\n",
      "average recall score: 0.37666666666666665\n",
      "average f1 score: 0.37666666666666665\n",
      "average cv score: 0.5085199555434283\n",
      "average precision score: 0.49666666666666665\n",
      "average recall score: 0.49666666666666665\n",
      "average f1 score: 0.49666666666666665\n"
     ]
    }
   ],
   "source": [
    "#classifier model with optimal settings\n",
    "classifier_NN = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes= (70, 50, 100), random_state=1)\n",
    "classifier_NB = MultinomialNB()\n",
    "classifier_LR = LogisticRegression(penalty = 'l2', C = 1.0)\n",
    "classifier_AB = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2), n_estimators=50)\n",
    "classifier_SVC = SVC(C=1.0, kernel = 'linear')\n",
    "\n",
    "models = [classifier_NN, classifier_NB, classifier_LR,classifier_AB, classifier_SVC]\n",
    "\n",
    "# transform the documents in the testing data to a document-term matrix\n",
    "X_test_vectorized = vector.transform(X_test)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_test_vectorized, y_test,test_size = 0.3)\n",
    "\n",
    "#Output the precision, recall and f1-score\n",
    "for model in models:\n",
    "    model.fit(X_train, y_train)  \n",
    "    pred = model.predict(X_val)\n",
    "    scores = cross_val_score(model, X_train, y_train, cv = 5)\n",
    "    prescore = precision_score(pred, y_val, average='micro')\n",
    "    rescore = recall_score(pred, y_val, average='micro')\n",
    "    f1score = f1_score(pred, y_val, average='micro')\n",
    "\n",
    "    print('average cv score:', sum(scores)/len(scores))\n",
    "    print('average precision score:', prescore)\n",
    "    print('average recall score:', rescore)\n",
    "    print('average f1 score:', f1score)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   score 1,  score 2,   score 3,   score 4,   socre 5\n",
      "precision score: [0.57142857 0.51612903 0.27777778 0.37096774 0.71186441]\n",
      "recall score:    [0.52941176 0.47058824 0.30612245 0.60526316 0.54545455]\n"
     ]
    }
   ],
   "source": [
    "classifier_LR = LogisticRegression(penalty = 'l2', C = 1.0)\n",
    "\n",
    "X_test_vectorized = vector.transform(X_test)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_test_vectorized, y_test,test_size = 0.3)\n",
    "\n",
    "classifier_LR.fit(X_train, y_train)  \n",
    "pred = classifier_LR.predict(X_val)\n",
    "\n",
    "precision = precision_score(pred, y_val, average=None)\n",
    "recall = recall_score(pred, y_val, average=None)\n",
    "print('                   score 1,  score 2,   score 3,   score 4,   socre 5')\n",
    "print('precision score:', precision)\n",
    "print('recall score:   ', recall)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the result, the rating score of the review is 3, the precision and recall is not good. That means if the the reviews are neural, don't contain sentiment(positive or negative) words, then they are hard to predict the rating score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5\n",
    "## Discuss\tsome ideas that could help improve\tyour prediction. \n",
    "### 1. Use sentiment analysis, we capture semantic similarities between words, and capture the sentiments of individual words within a review, in this way, it may help improve the accuracy of prediction. \n",
    "### 2. Use natural language processing method, such as Latent Semantic Analysis, LSA can finds hidden relationship between words in order to improve information  understanding. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
